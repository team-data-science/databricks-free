# 1.1 Course Overview & What You Will Build

Welcome to this course on Databricks Declarative Pipelines and Lakeflow Designer!

In this course, you'll learn how to build modern data pipelines in Databricks using a declarative approach - which means you define what you want, and Databricks handles the how.

## What You'll Learn

We'll start with the fundamentals: Delta Lake for reliable storage with versioning and ACID transactions, and Unity Catalog for data governance and discovery.

You'll discover how the medallion architecture - Bronze, Silver, Gold layers - progressively improves data quality from raw ingestion to business-ready metrics. This is the design pattern Databricks recommends for building scalable data systems.

Then you'll build an actual pipeline using a real e-commerce dataset with over 500,000 transactions. You'll transform raw CSV data through Bronze (ingestion), Silver (cleaning and modeling), and Gold (business metrics) layers.

Along the way, you'll use AI to accelerate your work. Databricks Assistant will help generate transformation logic. Genie will let you explore data using natural language. And Lakeflow Designer - an AI-powered, low-code pipeline builder - will help you build pipelines visually.

By the end, you'll have a complete pipeline that ingests data, cleans it, aggregates it into daily sales metrics. and visualizes the trends of daily sales. 

You will also learn how to handle streaming data from AWS.

## Who This Course Is For

This course is for data engineers, analysts, or anyone who wants to build production-grade data pipelines without writing tons of boilerplate code. You should have basic SQL and Python knowledge, but you don't need to be a Spark expert.

Ready to get started? Let's dive in.